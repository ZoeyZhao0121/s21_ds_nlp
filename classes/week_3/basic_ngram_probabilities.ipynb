{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-Gram Models and their probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first step in making use of n-gram models is using them to estimate the probabilities of certain events.\n",
    "\n",
    "In order to make use of these models we will first want to count the words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.util import bigrams \n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "dir_base = \"/Users/teacher/repos/f20_ds_nlp/classes/week_3/data/\"\n",
    "\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = open(filename , encoding='utf-8').read()\n",
    "    return input_file_text\n",
    "\n",
    "    \n",
    "def read_directory_files(directory):\n",
    "    file_texts = \"\"\n",
    "    files = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    for f in files:\n",
    "        file_texts += read_file(join(directory, f) )\n",
    "    return file_texts\n",
    "    \n",
    "text_corpus = read_directory_files(dir_base)\n",
    "print(text_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a single text variable that represents all the text in a corpus.\n",
    "\n",
    "Can we get counts and what not of this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(text_corpus)\n",
    "freq_bi = nltk.FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_bi.plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the problem with this data?\n",
    "\n",
    "Characters only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punkt_sentences = sentence_tokenizer.tokenize(text_corpus)\n",
    "sentences_words = [treebank_tokenizer.tokenize(sentence) for sentence in punkt_sentences]\n",
    "all_tokens = [word for sentence in sentences_words for word in sentence]\n",
    "bigrams = nltk.bigrams(all_tokens)\n",
    "freq_bi = nltk.FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freq_bi.most_common(20))\n",
    "freq_bi.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "content = [w for w in all_tokens if w.lower() not in stop_words]\n",
    "bigrams = nltk.bigrams(content)\n",
    "freq_bi = nltk.FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freq_bi.most_common(20))\n",
    "freq_bi.plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other things might we remove?\n",
    "\n",
    "Numbers? Punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freq_bi.hapaxes())\n",
    "print(freq_bi.N())\n",
    "print(freq_bi.freq(('May','21')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving from frequency counts to probabilities\n",
    "\n",
    "Above we used the NLTK library to count the number of tokens and n-grams. We can now fit a probability distribution to this data to determine the probability of different tokens and n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the Maximum Likelihood Estimator to construct \n",
    "# a probability distribution for things.\n",
    "MLE_Dist = nltk.MLEProbDist(freq_bi)\n",
    "print(MLE_Dist.max()) # tell us what the most likely item is.\n",
    "print(MLE_Dist.prob(('Air', 'Force'))) # regular probability\n",
    "print(MLE_Dist.logprob(('Air', 'Force')) ) # log probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we use words that we've never seen before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MLE_Dist.prob(('Chair', 'Force')))\n",
    "print(MLE_Dist.logprob(('Chair', 'Force')) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that we can construct a probability distribution, we can also smooth the distribution to avoid the zero probability problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the frequency counts and fit them into a \n",
    "# probability with Laplacean smoothing\n",
    "Smoothed_dist = nltk.LaplaceProbDist(freq_bi)\n",
    "print(Smoothed_dist.prob(('Chair', 'Force')))\n",
    "print(Smoothed_dist.logprob(('Chair', 'Force')) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate random strings from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Smoothed_dist.generate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine you have a sentence and you want to see about identifying its source. You have two separate language models you've constructed and so you are going to calculate the probability that a novel sentence has come from one of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_sentence_1 = \"USPACOM, in coordination with other DoD elements, government agencies, and partner nations, executes this contingency operation as a counterterrorism campaign supporting the Republic of the Philippines. \"\n",
    "test_sentence_2 = \"Quite unexpectedly, my curiosity turned into a foray into postmodern philosophy and critical theory, reflexivity and voice, various vague approaches to autobiographical inquiry, validity and acceptability, defences and criticisms, and a wide range of published personal narratives, the typical product of autoethnography.\"\n",
    "\n",
    "def get_sentence_bigrams(sentence):\n",
    "    sentence_words = treebank_tokenizer.tokenize(sentence)\n",
    "    word_count = len(sentence_words)\n",
    "    bigrams = nltk.bigrams(sentence_words)\n",
    "    return bigrams, word_count\n",
    "    \n",
    "def estimate_sentence_probability(bigram_sentence, word_length):\n",
    "    slogprob = 0\n",
    "    for bigram_words in bigram_sentence:\n",
    "        logprob= Smoothed_dist.logprob(bigram_words)\n",
    "        slogprob += logprob\n",
    "     \n",
    "    return slogprob/word_length\n",
    "\n",
    "bigram_sentence,word_count = get_sentence_bigrams(test_sentence_1)\n",
    "estimate_probability = estimate_sentence_probability(bigram_sentence, word_count)\n",
    "print(estimate_probability)\n",
    "print(np.exp(estimate_probability))\n",
    "\n",
    "\n",
    "\n",
    "bigram_sentence_2,word_count = get_sentence_bigrams(test_sentence_2)\n",
    "estimate_probability_2 = estimate_sentence_probability(bigram_sentence_2, word_count)\n",
    "print(estimate_probability_2)\n",
    "print(np.exp(estimate_probability_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these numbers look right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_better_sentence_bigrams(sentence):\n",
    "    sentence_words = treebank_tokenizer.tokenize(sentence)\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in sentence_words if w.lower() not in stop_words]\n",
    "    word_count = len(sentence_words)\n",
    "    bigrams = nltk.bigrams(content)\n",
    "    return bigrams, word_count\n",
    "    \n",
    "\n",
    "bigram_sentence, word_count = get_better_sentence_bigrams(test_sentence_1)\n",
    "estimate_probability = estimate_sentence_probability(bigram_sentence, word_count)\n",
    "print(estimate_probability)\n",
    "print(np.exp(estimate_probability))\n",
    "\n",
    "\n",
    "bigram_sentence, word_count = get_better_sentence_bigrams(test_sentence_2)\n",
    "estimate_probability = estimate_sentence_probability(bigram_sentence, word_count)\n",
    "print(estimate_probability)\n",
    "print(np.exp(estimate_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
