{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will be analyzing a number of articles. These articles are derived from a number of different news sections. For example, some will be from the politics section and some will be from entertainment, etc. Some documents will be somewhat duplicative.\n",
    "\n",
    "What we would like to do is get an idea of the type of vocabulary employed across all the documents. The hope is that by knowing the vocabulary across these documents we might identify certain words more associated with particular topics. Thus, we might expect that enterntainment might have the word \"film\" whereas politics might have the word \"vote.\"\n",
    "\n",
    "So we will want to generate several pieces of data. Your code should produce this information:\n",
    "- we want a list of all the unique words in this document corpus (You will want to consider lowercasing, stemming, and potentially lemmatization)\n",
    "- we want a list of the words in this document ranked by their frequency\n",
    "- we want a list of the words average frequency in documents (this indicates on average how many times does this term occur in a document\n",
    "- we want to identify what words documents contain unique words in the corpus (imagine a document mentioning a person with a distinct last name. It's possible that that name might only occur once in one document. We want to find all those documents.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use spacy\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_base = \"/homeworks/homework_1/data\" \n",
    "# point this to the data directory\n",
    "\n",
    "# you can use the below code to read all of the text files and then have them available in a list\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = open(filename, encoding='utf-8').read()\n",
    "    return input_file_text\n",
    "\n",
    "    \n",
    "def read_directory_files(directory):\n",
    "    file_texts = []\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    for f in files:\n",
    "        file_text = read_file(os.path.join(directory, f))\n",
    "        file_texts.append({\"file\":f, \"content\": file_text })\n",
    "    return file_texts\n",
    "    \n",
    "# here we will generate the list that contains all the files and their contents\n",
    "text_corpus = read_directory_files(dir_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all the words in a document\n",
    "def document_words(document):\n",
    "    word_list = []\n",
    "    analyzed = nlp(document)\n",
    "    for token in analyzed:\n",
    "        print(token)\n",
    "        # you will want to look at spacy's documentation on token properties\n",
    "        # https://spacy.io/api/token/\n",
    "        # This has the properties you can use in handling the individual tokens\n",
    "        \n",
    "        # here add to the word_list list\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract all the words in a document\n",
    "def word_frequency(document):\n",
    "    document_word_frequency_list = []\n",
    "    analyzed = nlp(document)\n",
    "    for token in analyzed:\n",
    "        print(token)\n",
    "        # you will want to look at spacy's documentation on token properties\n",
    "        # https://spacy.io/api/token/\n",
    "        # This has the properties you can use in handling the individual tokens\n",
    "        \n",
    "        # here add to the word_list list\n",
    "        # you will want to have something to capture the frequency of the word inside the document here.\n",
    "        # you want the document word frequency list as you will want to be able to get document level count info\n",
    "    return document_word_frequency_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration example. This will iterate over every document.\n",
    "for doc in text_corpus:\n",
    "    print(doc[\"content\"]) # the corpus is stored in a hash here. You can get the text by looking at the content key\n",
    "    document_words(doc[\"content\"])\n",
    "    word_frequency(doc[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Iteration Example.\n",
    "# We don't want to iterate over every combination twice\n",
    "for index_i, doc_i in enumerate(text_corpus):\n",
    "    for index_j, doc_j in enumerate(text_corpus[index_i:]):\n",
    "        # here we will want to get the word list from doc_i and compare it to the word list from doc_j\n",
    "        # This should tell us which words are unique between the two\n",
    "        # We can then accumulate the unique words per document and at the end of iterating through all\n",
    "        # documents we should be able to get which words are actually unique in the document.\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put a brief report of your approach, your results, and any conclusions you can draw from this here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra credit: \n",
    "\n",
    "Try to identify potential irregular verbs automatically (abuse the fact that the lemmas and the surface forms of irregular verbs are typically different. For example \"eat\" and \"ate\" have similar lemmas but different surface forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
